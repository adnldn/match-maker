{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('org_names.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data = list(set(data))\n",
    "sample = data[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Matcher Approach. Using the sequence matching algorithm from the difflib standard package. This is applied directly onto the untokenised strings themselves. Preprocessing involved removing common organisation suffixes using the basename function from the cleanco package. Each entry is compared against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from cleanco import basename\n",
    "\n",
    "def likely_related(names, threshold=0.6):\n",
    "    result = set()\n",
    "    for i in range(len(names)):\n",
    "        name1 = basename(basename(names[i].lower()))\n",
    "        for j in range(i+1, len(names)):\n",
    "            name2 = basename(basename(names[j].lower()))\n",
    "            similarity = SequenceMatcher(None, name1, name2).ratio()\n",
    "            if similarity >= threshold:\n",
    "                pair = tuple(sorted((names[i], names[j])))\n",
    "                result.add(pair)\n",
    "    return result\n",
    "\n",
    "likely_related(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I repeat the approach followed above but replace SequenceMatcher with a order invariant fuzzy matcher from TheFuzz package. Note that thefuzz makes use of difflib's SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "from cleanco import basename\n",
    "\n",
    "def likely_related(names, threshold=0.6):\n",
    "    result = set()\n",
    "    for i in range(len(names)):\n",
    "        name1 = basename(basename(names[i].lower()))\n",
    "        for j in range(i+1, len(names)):\n",
    "            name2 = basename(basename(names[j].lower()))\n",
    "            similarity = fuzz.token_sort_ratio(name1, name2)/100\n",
    "            if similarity >= threshold:\n",
    "                pair = tuple(sorted((names[i], names[j])))\n",
    "                result.add(pair)\n",
    "    return result\n",
    "\n",
    "likely_related(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I try an embedding based method. I begin with the same cleanco preprocesssing as in the above example, but I follow with tokenisation and vectorisation. This is done so that I can compute the cosine between individual examples and the entire corpus. This methods aims to cut down on the complexity by only having us compare individually against the top hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from cleanco import basename\n",
    "\n",
    "\n",
    "# mps else cpu\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "def likely_related(names, threshold=0.6):\n",
    "\n",
    "    names = [basename(basename(name)) for name in names]\n",
    "\n",
    "    vec = TfidfVectorizer(lowercase=True, analyzer=\"char\", ngram_range=(2, 3))\n",
    "    vec.fit(names)\n",
    "\n",
    "    names_tfidf = vec.transform(names)\n",
    "    names_tensor = torch.tensor(names_tfidf.toarray().astype(np.float32)).to(device)\n",
    "\n",
    "    result = set()\n",
    "    for i, name in enumerate(names):\n",
    "        name_tfidf = vec.transform([name])\n",
    "        name_tensor = torch.tensor(name_tfidf.toarray().astype(np.float32)).to(device)\n",
    "\n",
    "        similarity = F.cosine_similarity(name_tensor, names_tensor)\n",
    "\n",
    "        idxs = torch.where(similarity > threshold)[0]\n",
    "        for j in idxs:\n",
    "            if j != i:\n",
    "                pair = tuple(sorted((name, names[j])))\n",
    "                result.add(pair)\n",
    "\n",
    "    return result\n",
    "\n",
    "likely_related(data)\n",
    "\n",
    "# ADD BATCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I moved onto more modern transformer based embeddings. I considered several models from Hugging Face before settling on an SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from cleanco import basename\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "def likely_related(names, threshold=0.8):\n",
    "\n",
    "    names = [basename(name) for name in names]\n",
    "\n",
    "    model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "    # model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    # model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "    \n",
    "    embeddings = model.encode(names, convert_to_tensor=True).to(device)\n",
    "\n",
    "    # similarity = F.cosine_similarity(embeddings, embeddings)\n",
    "    similarity = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    result = set()\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i+1, len(names)):\n",
    "            if similarity[i][j] >= threshold:\n",
    "                pair = tuple(sorted((names[i], names[j])))\n",
    "                result.add(pair)\n",
    "\n",
    "    return result\n",
    "\n",
    "likely_related(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I settled on this solution. I then refactored it as a class and made it more compatitble with PyTorch's data handling to allow for more robust unit testing, batch processing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Final Solution. \"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from cleanco import basename\n",
    "\n",
    "def initialise_match_maker(model='distilbert-base-nli-stsb-mean-tokens', threshold=0.8, batch_size=64, device='cpu'):\n",
    "    \"\"\" Wrapper to select encoder model. \"\"\"\n",
    "    return MatchMaker(model, threshold, batch_size, device)\n",
    "\n",
    "class OrgNamesDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Organisation names dataset for PyTorch DataLoader.\n",
    "    Performs set to remove duplicates.\n",
    "    Optional transform to remove business suffixes, stopwords etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, transform=None):\n",
    "        with open(filename, 'r') as file:\n",
    "            names = json.load(file)\n",
    "            self.names = list(set(names))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        if self.transform is not None:\n",
    "            name = self.transform(name)\n",
    "        return name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "\n",
    "class MatchMaker:\n",
    "    def __init__(self, model, threshold, batch_size, device):\n",
    "        self.model = SentenceTransformer(model, device=torch.device(device))\n",
    "        self.threshold = threshold\n",
    "        self.batchsize = batch_size\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "    def similarity(self, embeddings):\n",
    "        \"\"\" Calculate similarity based on cosine. \"\"\"\n",
    "        return util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    def create_embeddings(self, names):\n",
    "        \"\"\" Load each batch and create embeddings of names. \"\"\"\n",
    "        dataloader = DataLoader(names, batch_size=self.batchsize, shuffle=False)\n",
    "\n",
    "        embeddings = [self.model.encode(batch, convert_to_tensor=True) for batch in dataloader]\n",
    "        embeddings = torch.cat(embeddings, dim=0).to(self.device)\n",
    "        return embeddings\n",
    "    \n",
    "    def pair(self, names):\n",
    "        \"\"\" Create list of pairs from embeddings. \"\"\"\n",
    "        embeddings = self.create_embeddings(names)\n",
    "        similarity = self.similarity(embeddings)\n",
    "\n",
    "        pairs = set()\n",
    "        for i in range(len(names)):\n",
    "            for j in range(i+1, len(names)):\n",
    "                if similarity[i][j] >= self.threshold:\n",
    "                    pair = tuple(sorted((names[i], names[j])))\n",
    "                    pairs.add(pair)\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OrgNamesDataset('org_names.json', transform=basename)\n",
    "subset_names = dataset.names[:10]\n",
    "match_maker = initialise_match_maker(model='distilbert-base-nli-stsb-mean-tokens', threshold=0.8, batch_size=64, device='cpu')\n",
    "pairs = match_maker.pair(dataset)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given more time, my next step would be to find a labelled data set with positive and negative examples of related names to fine-tune the chosen SBERT model. I also need to investigate how to best make use of the batches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
